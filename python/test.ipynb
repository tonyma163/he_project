{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274c3de7-9039-47c9-9cb5-fa782522cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, logging\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ec67a8-4628-4831-8a1d-044a5b128371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer & model\n",
    "logging.set_verbosity_error()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny-mnli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f660b11-e0f0-4978-8eaf-b0d583ac0861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8956/2088438174.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trained = torch.load('SST-2-BERT-tiny.bin', map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['bert.embeddings.position_ids'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load fine-tuned model weights\n",
    "trained = torch.load('SST-2-BERT-tiny.bin', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(trained, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "188544f0-f569-49ea-aaaf-3ac5bc020f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e314c82c-a022-4463-b215-41fe8195b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "text = \"Nuovo Cinema Paradiso has been an incredible movie! A gem in the italian culture.\"\n",
    "text = \"[CLS] \" + text + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be6d4e7e-52c8-43c9-8f17-6b9fdac907bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([[[ 0.7311, -0.0275, -8.5770,  ...,  1.3273,  0.1579, -0.1398],\n",
      "         [ 0.0530, -0.2743, -0.2208,  ..., -1.4017, -0.5093, -0.7404],\n",
      "         [-2.7270, -1.2554, -0.5465,  ..., -1.0224, -0.0679, -0.2995],\n",
      "         ...,\n",
      "         [-0.2928, -0.8412, -0.2279,  ..., -0.8374, -1.1446,  1.5150],\n",
      "         [ 0.3317,  0.4604, -0.2388,  ...,  0.6477, -0.1716, -2.4884],\n",
      "         [-1.4311,  0.4480, -0.1306,  ...,  1.0596, -1.3526, -0.9108]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input data\n",
    "#tokenized = tokenizer(text) # <- this would return the input_ids and attention mask as well\n",
    "tokenized_text = tokenizer.tokenize(text) # <- tokenize the input data\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) # <- convert tokens into ids\n",
    "tokens_tensor = torch.tensor([indexed_tokens]) # <- convert token ids into tensor\n",
    "\n",
    "#print(\"tokenized: \", tokenized)\n",
    "#print(\"tokenized_text: \", tokenized_text)\n",
    "#print(\"indexed_tokens: \", indexed_tokens)\n",
    "#print(\"tokens_tensor: \", tokens_tensor)\n",
    "\n",
    "# Embedding calculation\n",
    "x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])) # require token ids tensor and attention mask list\n",
    "\n",
    "print(\"x: \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea868c2c-a1c0-4b85-9cd0-6d7320d410d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embedding into text file\n",
    "path = \"./inputs/0\"\n",
    "for i in range(len(x[0])):\n",
    "    if not (os.path.exists(path)):\n",
    "        os.makedirs(path)\n",
    "    np.savetxt(\"./inputs/0/input_{}.txt\".format(i), x[0][i].detach(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd56318-0921-443c-89eb-5ea0c1e722ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1 - Self Attention\n",
    "# fetch the weights of Q, K, V\n",
    "query_weight = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "key_weight = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "value_weight = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "# fetch the biases of Q, K, V\n",
    "query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac399cbd-d3af-41c9-8a1e-9077e202d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot_product:  tensor([[[-0.3108, -0.0247,  0.8014,  ..., -0.2286,  1.3143, -0.9430],\n",
      "         [-0.6416, -0.5837,  0.7616,  ..., -0.7451,  0.7322, -0.8307],\n",
      "         [ 0.1578,  0.1372,  1.0732,  ..., -0.7571,  1.0376,  0.0132],\n",
      "         ...,\n",
      "         [-0.5239,  0.3390,  0.1429,  ..., -0.2271,  0.8409, -0.3488],\n",
      "         [-0.1250,  0.4574,  0.2978,  ..., -0.3089,  1.2559, -0.6545],\n",
      "         [-0.2520,  0.2029,  0.5690,  ..., -0.1556,  1.0461, -0.7791]]],\n",
      "       dtype=torch.float64, grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# convert input embeddings into double\n",
    "input_tensor = x.double()\n",
    "\n",
    "# calculate the new Q, K, V based on the new input embeddings\n",
    "query = torch.matmul(input_tensor, query_weight) + query_bias\n",
    "key = torch.matmul(input_tensor, key_weight) + key_bias\n",
    "value = torch.matmul(input_tensor, value_weight) + value_bias\n",
    "\n",
    "# reshape matrics for multi-head attention\n",
    "query = query.reshape([1, input_tensor.size()[1], 2, 64]) # 2 -> no. of attention heads, 64 -> dimension of each head (hidden size/no. of attention heads)\n",
    "key = key.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "value = value.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "# permute to adjust the dimensions of the tensors for the dot product operations\n",
    "query = query.permute([0, 2, 1, 3])\n",
    "key = key.permute([0, 2, 3, 1])\n",
    "\n",
    "# (Q * K) / square root of dk\n",
    "qk = torch.matmul(query, key)\n",
    "qk = qk / 8 # 1 / square root of 64\n",
    "\n",
    "# Softmax()\n",
    "qk_softmaxed = torch.softmax(qk, -1)\n",
    "\n",
    "# permute value\n",
    "value = value.permute([0, 2, 1, 3])\n",
    "\n",
    "# \n",
    "dot_product = torch.matmul(qk_softmaxed, value)\n",
    "# permute & reshape for further layer operations\n",
    "dot_product = dot_product.permute([0, 2, 1, 3])\n",
    "dot_product = dot_product.reshape([1, input_tensor.size()[1], 128])\n",
    "\n",
    "print(\"dot_product: \", dot_product)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
